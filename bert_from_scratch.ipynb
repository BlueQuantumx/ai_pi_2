{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from torch import optim, nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from bert import BertForClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = \"./aclImdb\"\n",
    "EPOCH = 5\n",
    "MAX_POSITION_EMBEDDINGS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, mode: Literal[\"train\", \"test\"]):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        for k, v in {\"{mode}/neg\": 0, \"{mode}/pos\": 1}.items():\n",
    "            class_root = os.path.join(DATABASE_PATH, k.format(mode=mode))\n",
    "            for j in os.listdir(class_root):\n",
    "                self.data.append(open(os.path.join(class_root, j)).read().strip())\n",
    "                self.label.append(v)\n",
    "        self.tokenized = tokenize(self.data).data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized[\"input_ids\"][idx]\n",
    "        attention_mask = self.tokenized[\"attention_mask\"][idx]\n",
    "        labels = self.label[idx]\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_dataset = ImdbDataset(\"train\")\n",
    "test_dataset = ImdbDataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertForClassification(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=768,\n",
    "    intermediate_size=4 * 768,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=8,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    num_hidden_layers=12,\n",
    "    num_labels=2,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5)\n",
    "# lr_scheduler\n",
    "\n",
    "bert.train()\n",
    "for epoch in range(EPOCH):\n",
    "    tqdm_bar = tqdm.tqdm(train_dataloader)\n",
    "    for input_ids, attention_mask, labels in train_dataloader:\n",
    "        labels: Tensor\n",
    "        input_ids = torch.stack(input_ids).to(\"cuda\")\n",
    "        attention_mask = torch.stack(attention_mask).to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        loss, logits = bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss: Tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        tqdm_bar.update(1)\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "torch.save(bert.state_dict(), \"bert_from_scratch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.load_state_dict(\n",
    "    torch.load(\"bert_from_scratch.pt\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "bert.eval()\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "for input_ids, attention_mask, labels in eval_dataloader:\n",
    "    labels: Tensor\n",
    "    input_ids = torch.stack(input_ids)  # .to(\"cuda\")\n",
    "    attention_mask = torch.stack(attention_mask)  # .to(\"cuda\")\n",
    "    labels = labels  # .to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        _, logits = bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
